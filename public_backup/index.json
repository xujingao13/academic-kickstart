[{"authors":["admin"],"categories":null,"content":"Jingao Xu is currently a Ph.D. candidate at School of Software, Tsinghua University, under supervision of Prof. Yunhao Liu and Prof. Zheng Yang since 2017. Prior to that, he received his B.E. degree in Software Engineering in 2017 from School of Software at Tsinghua University.\n Location: 11-238 East Main Building, Tsinghua University, Beijing, China\n E-mail: xujingao13 AT gmail DOT com\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/~jingao/authors/admin/","section":"authors","summary":"Jingao Xu is currently a Ph.D. candidate at School of Software, Tsinghua University, under supervision of Prof. Yunhao Liu and Prof. Zheng Yang since 2017. Prior to that, he received his B.E. degree in Software Engineering in 2017 from School of Software at Tsinghua University.\n Location: 11-238 East Main Building, Tsinghua University, Beijing, China\n E-mail: xujingao13 AT gmail DOT com","tags":null,"title":"Jingao Xu | 徐京傲","type":"authors"},{"authors":["Jingao Xu","Erqun Dong","Hao Cao","Jianzhe Liang"],"categories":[],"content":"","date":1640241879,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640241879,"objectID":"0d6d828df98175d69ddab3d38197b884","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/project/slam/","publishdate":"2021-12-23T14:44:39+08:00","relpermalink":"/~jingao/project/slam/","section":"project","summary":"","tags":["SLAM"],"title":"Mobile Systems based on Visual SLAM","type":"project"},{"authors":["Jingao Xu","Hao Cao","Jialin Zhang","Erqun Dong"],"categories":[],"content":"","date":1640241871,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640241871,"objectID":"076eb4eb012dedbae202794861e4c9d6","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/project/edge/","publishdate":"2021-12-23T14:44:31+08:00","relpermalink":"/~jingao/project/edge/","section":"project","summary":"","tags":["Edge"],"title":"Real-time Services based on Edge Computing","type":"project"},{"authors":["Jingao Xu","Danyang Li","Guoxuan Chi","Jialin Zhang"],"categories":[],"content":"","date":1640241864,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640241864,"objectID":"2cec055720246f349a18b32de536dda9","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/project/localization/","publishdate":"2021-12-23T14:44:24+08:00","relpermalink":"/~jingao/project/localization/","section":"project","summary":"","tags":["Localization"],"title":"Wireless Localization based on Multi-modal Fusion","type":"project"},{"authors":["Jingao Xu","Guoxuan Chi","Zheng Yang","Danyang Li","Qian Zhang","Qiang Ma","Xin Miao"],"categories":[],"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621833051,"objectID":"1a781017a2205cec79a45da6bc91d475","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/publication/xu-2021-mobisys-followupar/","publishdate":"2021-05-24T05:10:51.764949Z","relpermalink":"/~jingao/publication/xu-2021-mobisys-followupar/","section":"publication","summary":"Existing smartphone-based Augmented Reality (AR) systems are able to render virtual effects on static anchors. However, today's solutions lack the ability to render follow-up effects attached to moving anchors since they fail to track the 6 degrees of freedom (6-DoF) poses of them. We find an opportunity to accomplish the task by leveraging sensors capable of generating sparse point clouds on smartphones and fusing them with vision-based technologies. However, realizing this vision is non-trivial due to challenges in modeling radar error distributions and fusing heterogeneous sensor data. This study proposes FollowUpAR, a framework that integrates vision and sparse measurements to track object 6-DoF pose on smartphones. We derive a physical-level theoretical radar error distribution model based on an in-depth understanding of its hardware-level working principles and design a novel factor graph competent in fusing heterogeneous data. By doing so, FollowUpAR enables mobile devices to track anchor's pose accurately. We implement FollowUpAR on commodity smartphones and validate its performance with 800,000 frames in a total duration of 15 hours. The results show that FollowUpAR achieves a remarkable rotation tracking accuracy of 2.3° with a translation accuracy of 2.9mm, outperforming most existing tracking systems and comparable to state-of-the-art learning-based solutions. FollowUpAR can be integrated into ARCore and enable smartphones to render follow-up AR effects to moving objects. ","tags":[],"title":"FollowUpAR: Enabling Follow-up Effects in Mobile AR Applications","type":"publication"},{"authors":["Danyang Li","Jingao Xu","Zheng Yang","Yumeng Lu","Qian Zhang","Xinglin Zhang"],"categories":[],"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621327546,"objectID":"ac326f40dcd5282f74adb71803bdadee","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/publication/li-2021-infocom-itoloc/","publishdate":"2021-05-18T08:51:57.762082Z","relpermalink":"/~jingao/publication/li-2021-infocom-itoloc/","section":"publication","summary":"Among numerous indoor localization systems, WiFi fingerprint-based localization has been one of the most attractive solutions, which is known to be free of extra infrastructure and specialized hardware. To push forward this approach for wide deployment, three crucial goals on delightful deployment ubiquity, high localization accuracy, and low maintenance cost are desirable. However, due to severe challenges about signal variation, device heterogeneity, and database degradation root in environmental dynamics, pioneer works usually make a trade-off among them. In this paper, we propose iToLoc, a deep learning based localization system that achieves all three goals simultaneously. Once trained, iToLoc will provide accurate localization service for everyone using different devices and under diverse network conditions, and automatically update itself to maintain reliable performance anytime. iToLoc is purely based on WiFi fingerprints without relying on specific infrastructures. The core components of iToLoc are a domain adversarial neural network and a co-training based semi-supervised learning framework. Extensive experiments across 7 months with 8 different devices demonstrate that iToLoc achieves remarkable performance with an accuracy of 1.92m and  95% localization success rate. Even 7 months after the original fingerprint database was established, the rate still maintains  90%, which significantly outperforms previous works.","tags":[],"title":"Train Once, Locate Anytime for Anyone: Adversarial Learning Based Wireless Localization","type":"publication"},{"authors":["Liang Dong","Jingao Xu","Guoxuan Chi","Danyang Li","Xinglin Zhang","Jianbo Li","Qiang Ma","Zheng Yang"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640151438,"objectID":"42a926aadf2129dfcaabba41644b36a6","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/publication/dongtosn-imac/","publishdate":"2021-12-22T05:37:18.532916Z","relpermalink":"/~jingao/publication/dongtosn-imac/","section":"publication","summary":"Smartphone localization is essential to a wide spectrum of applications in the era of mobile computing. The ubiquity of smartphone mobile cameras and surveillance ambient cameras holds promise for offering sub-meter accuracy localization services thanks to the maturity of computer vision techniques. In general, ambient-camera-based solutions are able to localize pedestrians in video frames at fine-grained, but the tracking performance under dynamic environments remains unreliable. On the contrary, mobile-camera-based solutions are capable of continuously tracking pedestrians; however, they usually involve constructing a large volume of image database, a labor-intensive overhead for practical deployment. We observe an opportunity of integrating these two most promising approaches to overcome above limitations and revisit the problem of smartphone localization with a fresh perspective. However, fusing mobile-camera-based and ambient-camera-based systems is non-trivial due to disparity of camera in terms of perspectives, parameters and incorrespondence of localization results. In this article, we propose iMAC, an integrated mobile cameras and ambient cameras based localization system that achieves sub-meter accuracy and enhanced robustness with zero-human start-up effort. The key innovation of iMAC is a well-designed fusing frame to eliminate disparity of cameras including a construction of projection map function to automatically calibrate ambient cameras, an instant crowd fingerprints model to describe user motion patterns, and a confidence-aware matching algorithm to associate results from two sub-systems. We fully implement iMAC on commodity smartphones and validate its performance in five different scenarios. The results show that iMAC achieves a remarkable localization accuracy of 0.68 m, outperforming the state-of-the-art systems by 75%.","tags":[],"title":"Enabling Surveillance Cameras to Navigate","type":"publication"},{"authors":["Guoxuan Chi","Jingao Xu","Jialin Zhang","Qian Zhang","Qiang Ma","Zheng Yang"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640151132,"objectID":"d7a9acfd0659a15e9668f23731cc6d0b","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/publication/chi-2020-tmc-isat/","publishdate":"2021-12-22T05:32:11.895752Z","relpermalink":"/~jingao/publication/chi-2020-tmc-isat/","section":"publication","summary":"Indoor navigation is essential to a wide spectrum of applications in the era of mobile computing. Existing vision-based technologies suffer from both start-up costs and the absence of semantic information for navigation. We observe an opportunity to leverage pervasively deployed surveillance cameras to deal with the above drawbacks and revisit the problem of indoor navigation with a fresh perspective. In this paper, we propose iSAT, a system that enables public surveillance cameras, as indoor navigating satellites, to locate users on the floorplan, tell users with semantic information about the surrounding environment, and guide users with navigation instructions. However, enabling public cameras to navigate is non-trivial due to 3 factors: absence of real scale, disparity of camera perspective, and lack of semantic information. To overcome these challenges, iSAT leverages POI-assisted framework and adopts a novel coordinate transformation algorithm to associate public and mobile cameras, and further attaches semantic information to user location. Extensive experiments in 4 different scenarios show that iSAT achieves a localization accuracy of 0.48m and a navigation success rate of 90.5%, outperforming the state-of-the-art systems by over 30%. Benefitting from our solution, all areas with public cameras can upgrade to smart spaces with visual navigation services.","tags":[],"title":"Locate, Tell, and Guide: Enabling Public Cameras to Navigate the Public","type":"publication"},{"authors":["Jingao Xu","Erqun Dong","Qiang Ma","Chenshu Wu","Zheng Yang"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621833052,"objectID":"0b6c5f00c94581e1dd380fdddfbddb80","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/publication/xu-2021-smartphone/","publishdate":"2021-05-24T05:10:51.994535Z","relpermalink":"/~jingao/publication/xu-2021-smartphone/","section":"publication","summary":"Existing indoor navigation solutions usually require pre-deployed comprehensive location services with precise indoor maps and, more importantly, all rely on dedicatedly installed or existing infrastructure. In this article, we present Pair-Navi, an infrastructure-free indoor navigation system that circumvents all these requirements by reusing a previous traveler’s (i.e., leader) trace experience to navigate future users (i.e., followers) in a Peer-to-Peer mode. Our system leverages the advances of visual simultaneous localization and mapping (SLAM) on commercial smartphones. Visual SLAM systems, however, are vulnerable to environmental dynamics in the precision and robustness and involve intensive computation that prohibits real-time applications. To combat environmental changes, we propose to cull non-rigid contexts and keep only the static and rigid contents in use. To enable real-time navigation on mobiles, we decouple and reorganize the highly coupled SLAM modules for leaders and followers. We implement Pair-Navi on commodity smartphones and validate its performance in three diverse buildings and two standard datasets (TUM and KITTI). Our results show that Pair-Navi achieves an immediate navigation success rate of 98.6%, which maintains as 83.4% even after 2 weeks since the leaders’ traces were collected, outperforming the state-of-the-art solutions by 50%. Being truly infrastructure-free, Pair-Navi sheds lights on practical indoor navigations for mobile users.","tags":[],"title":"Smartphone-Based Indoor Visual Navigation with Leader-Follower Mode","type":"publication"},{"authors":["Danyang Li","Jingao Xu","Zheng Yang","Chenshu Wu","Jianbo Li","Nicholas D Lane"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640151986,"objectID":"14d2dbfae0f780aa33b43d4cd3e2db1b","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/publication/li-2021-wireless/","publishdate":"2021-12-22T05:46:25.831048Z","relpermalink":"/~jingao/publication/li-2021-wireless/","section":"publication","summary":"Indoor localization has gained increasing attention in the era of the Internet of Things. Among various technologies, WiFi fingerprint-based localization has become a mainstream solution. However, RSS fingerprints suffer from critical drawbacks of spatial ambiguity and temporal instability that root in multipath effects and environmental dynamics, which degrade the performance of these systems and therefore impede their wide deployment in the real world. Pioneering works overcome these limitations at the costs of ubiquity as they mostly resort to additional information or extra user constraints. In this article, we present the design and implementation of ViViPlus, an indoor localization system purely based on WiFi fingerprints, which jointly mitigates spatial ambiguity and temporal instability and derives reliable performance without impairing the ubiquity. The key idea is to embrace the spatial awareness of RSS values in a novel form of RSS Spatial Gradient (RSG) matrix for enhanced WiFi fingerprints. We devise techniques for the representation, construction, and localization of the proposed fingerprint form and integrate them all in a practical system. Extensive experiments across 7 months in different environments demonstrate that ViViPlus significantly improves the accuracy in localization scenarios by about 30% to 50% compared with the state-of-the-art approaches.","tags":[],"title":"Wireless Localization with Spatial-Temporal Robust Fingerprints","type":"publication"},{"authors":["Erqun Dong","Jianzhe Liang","Zeyu Wang","Jingao Xu","Longfei Shangguan","Qiang Ma","Zheng Yang"],"categories":[],"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621327589,"objectID":"2e52c467d1509a598862a7b43edf5c27","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/publication/dong-2020-icpads-gmnp/","publishdate":"2021-05-18T08:51:57.979271Z","relpermalink":"/~jingao/publication/dong-2020-icpads-gmnp/","section":"publication","summary":"Visual peer-to-peer navigation is a suitable solution for indoor navigation for it relieves the labor of site-survey and eliminates infrastructure dependence. However, a major drawback hampers its application, as the peer-to-peer mode suffers from a deficiency of paths in large indoor scenarios with multifarious places-of-interest. Nevertheless, we propose one with a profound crowdsourcing scheme that addresses the drawback by merging the paths of different leaders' into a global map. To realize the idea, we further deal with entailed challenges, namely the unidirectional disadvantage, the scale ambiguity, and large computational overhead. We design a navigation strategy to solve the unidirectional problem and turn to VIO to tackle scale ambiguity. We devise a mobile-edge architecture to enable real-time navigation (30fps, 100ms end-to-end delay) and lighten the burden of smartphones (35% battery life for 2h35min) while assuring the accuracy of localization and map construction. Through experimental validations, we show that P2P navigation, previously relying on the abundance of independent paths, can enjoy a sufficiency of navigation paths with a crowdsourced global map. The experiments demonstrate a navigation success rate of 100% and spatial offset of less than 3.2m, better than existing works.","tags":[],"title":"Improving the Applicability of Visual Peer-to-Peer Navigation with Crowdsourcing","type":"publication"},{"authors":["Liang Dong","Jingao Xu","Guoxuan Chi","Danyang Li","Xinglin Zhang","Jianbo Li","Qiang Ma","Zheng Yang"],"categories":[],"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621327735,"objectID":"206b0e6d7025cea2c226a6827e88ebed","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/publication/dong-2020-icccn-imac/","publishdate":"2021-05-18T08:51:58.409415Z","relpermalink":"/~jingao/publication/dong-2020-icccn-imac/","section":"publication","summary":"Smartphone localization is essential to a wide spectrum of applications in the era of mobile computing. The ubiquity of smartphone mobile cameras and surveillance ambient cameras holds promise for offering sub-meter accuracy localization services thanks to the maturity of computer vision techniques. In general, ambient-camera-based solutions are able to localize pedestrians in video frames at fine-grained, but the tracking performance under dynamic environments remains unreliable. On the contrary, mobile-camera-based solutions are capable of continuously tracking pedestrians, however, they usually involve constructing a large volume of image database, a labor-intensive overhead for practical deployment. We observe an opportunity of integrating these two most promising approaches to overcome above limitations and revisit the problem of smartphone localization with a fresh perspective. However, fusing mobile-camera-based and ambient-camera-based systems is non-trivial due to disparity of camera in terms of perspectives, parameters and incorrespondence of localization results. In this paper, we propose iMAC, an integrated mobile cameras and ambient cameras based localization system that achieves sub-meter accuracy and enhanced robustness with zero-human start-up effort. The key innovation of iMAC is a well-designed fusing frame to eliminate disparity of cameras including a construction of projection map function to automatically calibrate ambient cameras, an instant crowd fingerprints model to describe user motion patterns, and a confidence-aware matching algorithm to associate results from two sub-systems. We fully implement iMAC on commodity smart-phones and validate its performance in five different scenarios. The results show that iMAC achieves a remarkable localization accuracy of 0.68m, outperforming the state-of-the-art systems by  75%.","tags":[],"title":"Enabling Surveillance Cameras to Navigate","type":"publication"},{"authors":["Jingao Xu","Hao Cao","Danyang Li","Kehong Huang","Chen Qian","Longfei Shangguan","Zheng Yang"],"categories":[],"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621327718,"objectID":"5b36844a98510fac8eaf17079bf7b1f6","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/publication/xu-2020-infocom-edge-slam/","publishdate":"2021-05-18T08:51:58.197416Z","relpermalink":"/~jingao/publication/xu-2020-infocom-edge-slam/","section":"publication","summary":"Localization and navigation play a key role in many location-based services and have attracted numerous research efforts from both academic and industrial community. In recent years, visual SLAM has been prevailing for robots and autonomous driving cars. However, the ever-growing computation resource demanded by SLAM impedes its application to resource-constrained mobile devices. In this paper we present the design, implementation, and evaluation of edgeSLAM, an edge assisted real-time semantic visual SLAM service running on mobile devices. edgeSLAM leverages the state-of-the-art semantic segmentation algorithm to enhance localization and mapping accuracy, and speeds up the computation-intensive SLAM and semantic segmentation algorithms by computation offloading. The key innovations of edgeSLAM include an efficient computation offloading strategy, an opportunistic data sharing mechanism, and an adaptive task scheduling algorithm. We fully implement edgeSLAM on an edge server and different types of mobile devices (2 types of smartphones and a development board). Extensive experiments are conducted under 3 data sets, and the results show that edgeSLAM is able to run on mobile devices at 35fps frame rate and achieves a 5cm localization accuracy, outperforming existing solutions by more than 15%. We also demonstrate the usability of edgeSLAM through 2 case studies of pedestrian localization and robot navigation. To the best of our knowledge, edgeSLAM is the first real-time semantic visual SLAM for mobile devices.","tags":[],"title":"Edge Assisted Mobile Semantic Visual SLAM","type":"publication"},{"authors":["Jingao Xu","Hengjie Chen","Kun Qian","Erqun Dong","Min Sun","Chenshu Wu","Li Zhang","Zheng Yang"],"categories":[],"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621327918,"objectID":"980c61d5d773dd06eb2f9da738029b4a","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/publication/xu-2019-ubicomp-ivr/","publishdate":"2021-05-18T08:51:58.62353Z","relpermalink":"/~jingao/publication/xu-2019-ubicomp-ivr/","section":"publication","summary":"Smartphone localization is essential to a wide range of applications in shopping malls, museums, office buildings, and other public places. Existing solutions relying on radio fingerprints and/or inertial sensors suffer from large location errors and considerable deployment efforts. We observe an opportunity in the recent trend of increasing numbers of security surveillance cameras installed in indoor spaces to overcome these limitations and revisit the problem of smartphone localization with a fresh perspective. However, fusing vision-based and radio-based systems is non-trivial due to the absence of absolute location, incorrespondence of identification and looseness of sensor fusion. This study proposes iVR, an integrated vision and radio localization system that achieves sub-meter accuracy with indoor semantic maps automatically generated from only two surveillance cameras, superior to precedent systems that require manual map construction or plentiful captured images. iVR employs a particle filter to fuse raw estimates from multiple systems, including vision, radio, and inertial sensor systems. By doing so, iVR outputs enhanced accuracy with zero start-up costs, while overcoming the respective drawbacks of each individual sub-system. We implement iVR on commodity smartphones and validate its performance in five different scenarios. The results show that iVR achieves a remarkable localization accuracy of 0.7m, outperforming the state-of-the-art systems by 70%.","tags":[],"title":"iVR: Integrated Vision and Radio Localization with Zero Human Effort","type":"publication"},{"authors":["Erqun Dong","Jingao Xu","Chenshu Wu","Yunhao Liu","Zheng Yang"],"categories":[],"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621327919,"objectID":"da389d9348bcf60257231c64c8cdeb7f","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/publication/dong-2019-infocom-pairnavi/","publishdate":"2021-05-18T08:51:58.839614Z","relpermalink":"/~jingao/publication/dong-2019-infocom-pairnavi/","section":"publication","summary":"Existing indoor navigation solutions usually require pre-deployed comprehensive location services with precise indoor maps and, more importantly, all rely on dedicatedly installed or existed infrastructure. In this paper, we present Pair-Navi, an infrastructure-free indoor navigation system that circumvents all these requirements by reusing a previous traveler's (i.e. leader) trace experience to navigate future users (i.e. followers) in a Peer-to-Peer (P2P) mode. Our system leverages the advances of visual SLAM on commercial smartphones. Visual SLAM systems, however, are vulnerable to environmental dynamics in the precision and robustness and involve intensive computation that prohibits real-time applications. To combat environmental changes, we propose to cull non-rigid contexts and keep only the static and rigid contents in use. To enable real-time navigation on mobiles, we decouple and reorganize the highly coupled SLAM modules for leaders and followers. We implement Pair-Navi on commodity smartphones and validate its performance in three diverse buildings. Our results show that Pair-Navi achieves an immediate navigation success rate of 98.6%, which maintains as 83.4% even after two weeks since the leaders' traces were collected, outperforming the state-of-the-art solutions by 50%. Being truly infrastructure-free, Pair-Navi sheds lights on practical indoor","tags":[],"title":"Pair-Navi: Peer-to-Peer Indoor Navigation with Mobile Visual SLAM","type":"publication"},{"authors":["Jingao Xu","Zheng Yang","Hengjie Chen","Yunhao Liu","Xianchun Zhou","Jinbo Li","Nicholas Lane"],"categories":[],"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621413791,"objectID":"39201e89f9b666981ade4d61e1d72270","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/publication/mass-2018-xu-viviplus/","publishdate":"2021-05-19T08:43:11.387645Z","relpermalink":"/~jingao/publication/mass-2018-xu-viviplus/","section":"publication","summary":"Indoor localization gains increasingly attentions in the era of Internet of Things. Among various technologies, WiFi-based systems that leverage Received Signal Strengths (RSSs) as location fingerprints become the mainstream solutions. However, RSS fingerprints suffer from critical drawbacks of spatial ambiguity and temporal instability that root in multipath effects and environmental dynamics, which degrade the performance of these systems and therefore impede their wide deployment in real world. Pioneering works overcome these limitations at the costs of ubiquity as they mostly resort to additional information or extra user constraints. In this paper, we present the design and implementation of ViViPlus, an indoor localization system purely based on WiFi fingerprints, which jointly mitigates spatial ambiguity and temporal instability and derives reliable performance without impairing the ubiquity. The key idea is to embrace the spatial awareness of RSS values in a novel form of RSS Spatial Gradient (RSG) matrix for enhanced WiFi fingerprints. We devise techniques for the representation, construction, and comparison of the proposed fingerprint form, and integrate them all in a practical system, which follows the classical fingerprinting framework and requires no more inputs than any previous RSS fingerprint based systems. Extensive experiments in different environments demonstrate that ViViPlus significantly improves the accuracy in both localization and tracking scenarios by about 30% to 50% compared with five state-of-the-art approaches.","tags":[],"title":"Embracing Spatial Awareness for Reliable WiFi-Based Indoor Location Systems","type":"publication"},{"authors":["Chenshu Wu","Jingao Xu","Zheng Yang","Nicholas D. Lane","Zuwei Yin"],"categories":[],"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621327919,"objectID":"b61ba48e5817df24588becb0436fd60e","permalink":"http://tns.thss.tsinghua.edu.cn/~jingao/publication/wu-2017-ubicomp-vivi/","publishdate":"2021-05-18T08:51:59.065238Z","relpermalink":"/~jingao/publication/wu-2017-ubicomp-vivi/","section":"publication","summary":"Among numerous indoor localization systems proposed during the past decades, WiFi fingerprint-based localization has been one of the most attractive solutions, which is known to be free of extra infrastructure and specialized hardware. However, current WiFi fingerprinting suffers from a pivotal problem of RSS fluctuations caused by unpredictable environmental dynamics. The RSS variations lead to severe spatial ambiguity and temporal instability in RSS fingerprinting, both impairing the location accuracy. To overcome such drawbacks, we propose fingerprint spatial gradient (FSG), a more stable and distinctive form than RSS fingerprints, which exploits the spatial relationships among the RSS fingerprints of multiple neighbouring locations. As a spatially relative form, FSG is more resistant to RSS uncertainties. Based on the concept of FSG, we design novel algorithms to construct FSG on top of a general RSS fingerprint database and then propose effective FSG matching methods for location estimation. Unlike previous works, the resulting system, named ViVi, yields performance gain without the pains of introducing extra information or additional service restrictions or assuming impractical RSS models. Extensive experiments in different buildings demonstrate that ViVi achieves great performance, outperforming the best among four comparative start-of-the-art approaches by 29% in mean accuracy and 19% in 95th percentile accuracy and outweighing the worst one by 39% and 24% respectively. We envision FSG as a promising supplement and alternative to existing RSS fingerprinting for future WiFi localization.","tags":[],"title":"Gain Without Pain: Accurate WiFi-based Localization with Fingerprint Spatial Gradient","type":"publication"}]